{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune GPT on SageMaker Examples\n",
    "Generative Pre-trained Transformer. In this example, we'll fine-tune a large GPT-2 on the Amazon SageMaker Examples code repository.\n",
    "\n",
    "First, let's process the raw notebook files and convert them into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amazon-sagemaker-examples'...\n",
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 9830 (delta 2), reused 1 (delta 1), pack-reused 9821\u001b[K\n",
      "Receiving objects: 100% (9830/9830), 273.90 MiB | 43.44 MiB/s, done.\n",
      "Resolving deltas: 100% (5433/5433), done.\n",
      "Checking out files: 100% (1524/1524), done.\n"
     ]
    }
   ],
   "source": [
    "# consider doing a fresh clone so you all the content raw\n",
    "!git clone https://github.com/awslabs/amazon-sagemaker-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'your-bucket'\n",
    "path = 'your-prefix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked for  rl_knapsack_coach_customEnv.ipynb\n",
      "broke on  bandits_movielens_testbed.ipynb\n",
      "broke on  rl_news_vendor_ray_custom.ipynb\n",
      "broke on  rl_vehicle_routing_problem_ray_custom.ipynb\n",
      "broke on  rl_bin_packing_ray_custom.ipynb\n",
      "broke on  rl_cartpole_coach_gymEnv.ipynb\n",
      "broke on  rl_roboschool_ray_distributed.ipynb\n",
      "broke on  rl_roboschool_ray_automatic_model_tuning.ipynb\n",
      "broke on  rl_roboschool_ray.ipynb\n",
      "broke on  rl_roboschool_stable_baselines.ipynb\n",
      "broke on  bandits_statlog_vw_customEnv.ipynb\n",
      "broke on  rl_hvac_coach_energyplus.ipynb\n",
      "broke on  rl_cartpole_batch_coach.ipynb\n",
      "broke on  rl_predictive_autoscaling_coach_customEnv.ipynb\n",
      "broke on  rl_objecttracker_coach_robomaker.ipynb\n",
      "broke on  rl_managed_spot_cartpole_coach_gymEnv.ipynb\n",
      "broke on  deepracer_rl.ipynb\n",
      "worked for  rl_gamerserver_ray.ipynb\n",
      "worked for  rl_network_compression_ray_custom.ipynb\n",
      "worked for  rl_tic_tac_toe_coach_customEnv.ipynb\n",
      "broke on  rl_portfolio_management_coach_customEnv.ipynb\n",
      "broke on  rl_mountain_car_coach_gymEnv.ipynb\n",
      "broke on  rl_traveling_salesman_vehicle_routing_coach.ipynb\n",
      "broke on  tune_r_bring_your_own.ipynb\n",
      "broke on  r_xgboost_batch_transform.ipynb\n",
      "broke on  r_xgboost_hpo_batch_transform.ipynb\n",
      "worked for  r_in_sagemaker_processing.ipynb\n",
      "broke on  r_sagemaker_hello_world.ipynb\n",
      "worked for  r_sagemaker_abalone.ipynb\n",
      "worked for  batch_transform_pca_dbscan_movie_clusters.ipynb\n",
      "worked for  tensorflow-serving-cifar10-python-sdk.ipynb\n",
      "worked for  Batch Transform - breast cancer prediction with lowel level SDK.ipynb\n",
      "worked for  Batch Transform - breast cancer prediction with high level SDK.ipynb\n",
      "broke on  tensorflow-serving-jpg-python-sdk.ipynb\n",
      "broke on  tensorflow-serving-jpg-cli.ipynb\n",
      "broke on  working-with-tfrecords.ipynb\n",
      "broke on  pytorch-mnist-batch-transform.ipynb\n",
      "broke on  tensorflow-serving-tfrecord-python-sdk.ipynb\n",
      "broke on  tensorflow-serving-tfrecord.cli.ipynb\n",
      "worked for  pytorch_torchvision_neo.ipynb\n",
      "broke on  tensorflow_distributed_mnist_neo_inf1.ipynb\n",
      "worked for  Image-classification-fulltraining-highlevel-neo.ipynb\n",
      "worked for  mxnet_mnist_neo.ipynb\n",
      "broke on  xgboost_customer_churn_neo.ipynb\n",
      "worked for  pytorch-vgg19-bn.ipynb\n",
      "worked for  gluoncv_ssd_mobilenet_neo.ipynb\n",
      "worked for  tensorflow_distributed_mnist_neo.ipynb\n",
      "worked for  feature_transformation_with_sagemaker_processing.ipynb\n",
      "worked for  feature_transformation_with_sagemaker_processing_dask.ipynb\n",
      "worked for  scikit_learn_data_processing_and_model_evaluation.ipynb\n",
      "worked for  tf-mnist-builtin-rule.ipynb\n",
      "worked for  mnist_tensor_analysis.ipynb\n",
      "worked for  mxnet-spot-training-with-sagemakerdebugger.ipynb\n",
      "worked for  xgboost-regression-debugger-rules.ipynb\n",
      "worked for  mxnet-realtime-analysis.ipynb\n",
      "worked for  tf-keras-custom-rule.ipynb\n",
      "broke on  pytorch_byoc_smdebug.ipynb\n",
      "broke on  tf2-keras-custom-container.ipynb\n",
      "broke on  tf2-keras-default-container.ipynb\n",
      "worked for  tf-mnist-stop-training-job.ipynb\n",
      "worked for  detect_stalled_training_job_and_stop.ipynb\n",
      "worked for  mnist-tensor-plot.ipynb\n",
      "worked for  xgboost-realtime-analysis.ipynb\n",
      "worked for  xgboost-census-debugger-rules.ipynb\n",
      "worked for  cnn_class_activation_maps.ipynb\n",
      "broke on  bert_attention_head_view.ipynb\n",
      "worked for  autoencoder_mnist.ipynb\n",
      "broke on  iterative_model_pruning_resnet.ipynb\n",
      "broke on  iterative_model_pruning_alexnet.ipynb\n",
      "broke on  xgboost_mnist.ipynb\n",
      "worked for  object_detection_birds.ipynb\n",
      "worked for  object2vec_multilabel_genre_classification.ipynb\n",
      "worked for  semantic_segmentation_pascalvoc.ipynb\n",
      "worked for  Image-classification-transfer-learning-highlevel.ipynb\n",
      "broke on  Image-classification-fulltraining.ipynb\n",
      "worked for  Image-classification-lst-format.ipynb\n",
      "broke on  Image-classification-fulltraining-elastic-inference.ipynb\n",
      "worked for  Image-classification-incremental-training-highlevel.ipynb\n",
      "broke on  Image-classification-fulltraining-highlevel.ipynb\n",
      "broke on  Image-classification-transfer-learning.ipynb\n",
      "worked for  Image-classification-lst-format-highlevel.ipynb\n",
      "worked for  SageMaker-Seq2Seq-Translation-English-German.ipynb\n",
      "worked for  ipinsights-tutorial.ipynb\n",
      "worked for  object2vec_movie_recommendation.ipynb\n",
      "broke on  blazingtext_hosting_pretrained_fasttext.ipynb\n",
      "worked for  pca_mnist.ipynb\n",
      "worked for  factorization_machines_mnist.ipynb\n",
      "worked for  random_cut_forest.ipynb\n",
      "worked for  DeepAR-Electricity.ipynb\n",
      "worked for  object2vec_sentence_similarity.ipynb\n",
      "worked for  managed_spot_training_object_detection.ipynb\n",
      "worked for  ntm_synthetic.ipynb\n",
      "worked for  k_nearest_neighbors_covtype.ipynb\n",
      "broke on  LDA-Introduction.ipynb\n",
      "worked for  linear_learner_mnist_with_file_system_data_source.ipynb\n",
      "worked for  linear_learner_mnist.ipynb\n",
      "broke on  blazingtext_text_classification_dbpedia.ipynb\n",
      "broke on  blazingtext_word2vec_text8.ipynb\n",
      "worked for  object_detection_image_json_format.ipynb\n",
      "worked for  object_detection_incremental_training.ipynb\n",
      "worked for  object_detection_recordio_format.ipynb\n",
      "worked for  deepar_synthetic.ipynb\n",
      "broke on  Image-classification-multilabel-lst.ipynb\n",
      "broke on  blazingtext_word2vec_subwords_text8.ipynb\n",
      "broke on  xgboost_abalone_dist_script_mode.ipynb\n",
      "broke on  xgboost_managed_spot_training.ipynb\n",
      "worked for  xgboost_parquet_input_training.ipynb\n",
      "broke on  xgboost_abalone.ipynb\n",
      "worked for  fair_linear_learner.ipynb\n",
      "worked for  Breast Cancer Prediction.ipynb\n",
      "broke on  xgboost_customer_churn.ipynb\n",
      "broke on  sagemaker-countycensusclustering.ipynb\n",
      "worked for  xgboost_direct_marketing_sagemaker.ipynb\n",
      "broke on  ntm_20newsgroups_topic_model.ipynb\n",
      "broke on  EnsembleLearnerCensusIncome.ipynb\n",
      "worked for  linear_time_series_forecast.ipynb\n",
      "worked for  gluon_recommender_system.ipynb\n",
      "broke on  deepar_chicago_traffic_violations.ipynb\n",
      "broke on  object2vec_document_embedding.ipynb\n",
      "worked for  video-game-sales-xgboost.ipynb\n",
      "worked for  pyspark_mnist_pca_mllib_kmeans.ipynb\n",
      "worked for  pyspark_mnist_kmeans.ipynb\n",
      "worked for  pyspark_mnist_xgboost.ipynb\n",
      "worked for  pyspark_mnist_custom_estimator.ipynb\n",
      "worked for  pyspark_mnist_pca_kmeans.ipynb\n",
      "worked for  mxnet_onnx.ipynb\n",
      "worked for  tensorflow_moving_from_framework_mode_to_script_mode.ipynb\n",
      "worked for  mxnet_gcmc_hypertune.ipynb\n",
      "worked for  mxnet_gcmc.ipynb\n",
      "worked for  managed_spot_training_mxnet.ipynb\n",
      "worked for  tf-eager-sm-scriptmode.ipynb\n",
      "worked for  tensorflow_script_mode_using_shell_commands.ipynb\n",
      "worked for  kmeans_mnist_lowlevel.ipynb\n",
      "worked for  tensorboard_keras.ipynb\n",
      "worked for  mxnet_embedding_server.ipynb\n",
      "worked for  chainer_mnist_local_mode.ipynb\n",
      "worked for  pytorch_local_mode_cifar10.ipynb\n",
      "worked for  mxnet_onnx_eia.ipynb\n",
      "worked for  tensorflow_script_mode_pipe_mode.ipynb\n",
      "worked for  Bring Your Own DL Framework to Amazon Sagemaker with Model Server for Apache MXNet's (MMS) BYO container.ipynb\n",
      "worked for  tensorflow_serving_container.ipynb\n",
      "worked for  mxnet_mnist_with_gluon.ipynb\n",
      "worked for  mxnet_mnist_with_gluon_local_mode.ipynb\n",
      "worked for  mxnet_sentiment_analysis_with_gluon.ipynb\n",
      "worked for  managed_spot_training_tensorflow_estimator.ipynb\n",
      "worked for  pytorch_rnn.ipynb\n",
      "worked for  mxnet_mnist_elastic_inference.ipynb\n",
      "worked for  mxnet_mnist_elastic_inference_local.ipynb\n",
      "worked for  mxnet_mnist.ipynb\n",
      "worked for  mxnet_mnist_with_batch_transform.ipynb\n",
      "worked for  kmeans_mnist.ipynb\n",
      "broke on  Scikit-learn Estimator Example With Batch Transform.ipynb\n",
      "worked for  pytorch_gcn.ipynb\n",
      "broke on  mxnet_gcn_hypertune.ipynb\n",
      "worked for  mxnet_gcn.ipynb\n",
      "worked for  pytorch_gcn_hypertune.ipynb\n",
      "worked for  sparkml_serving_emr_mleap_abalone.ipynb\n",
      "worked for  tensorflow_script_mode_training_and_serving.ipynb\n",
      "worked for  tensorflow_script_mode_quickstart.ipynb\n",
      "worked for  kge_mxnet.ipynb\n",
      "worked for  kge_pytorch_hypertune.ipynb\n",
      "worked for  kge_mxnet_hypertune.ipynb\n",
      "worked for  kge_pytorch.ipynb\n",
      "broke on  pytorch_mnist_horovod.ipynb\n",
      "worked for  chainer_sentiment_analysis.ipynb\n",
      "worked for  pytorch_mnist.ipynb\n",
      "worked for  tensorflow_serving_pretrained_model_elastic_inference.ipynb\n",
      "broke on  Inference Pipeline with Scikit-learn and Linear Learner.ipynb\n",
      "worked for  tensorflow_script_mode_horovod.ipynb\n",
      "worked for  Sklearn_on_SageMaker_end2end.ipynb\n",
      "worked for  pytorch-gcn-tox21-hypertune.ipynb\n",
      "worked for  pytorch-gcn-tox21.ipynb\n",
      "worked for  tensorflow_keras_CIFAR10.ipynb\n",
      "worked for  mxnet_onnx_export.ipynb\n",
      "worked for  chainer_single_machine_cifar10.ipynb\n",
      "worked for  chainermn_distributed_cifar10.ipynb\n",
      "broke on  pipe_bring_your_own.ipynb\n",
      "worked for  inference_pipeline_sparkml_blazingtext_dbpedia.ipynb\n",
      "broke on  mask-rcnn-fsx.ipynb\n",
      "broke on  mask-rcnn-experiment-trials.ipynb\n",
      "broke on  mask-rcnn-inference.ipynb\n",
      "broke on  mask-rcnn-efs.ipynb\n",
      "broke on  mask-rcnn-s3.ipynb\n",
      "broke on  xgboost_bring_your_own_model.ipynb\n",
      "worked for  pytorch_extending_our_containers.ipynb\n",
      "worked for  r_bring_your_own.ipynb\n",
      "worked for  fastai_lesson1_sagemaker_example.ipynb\n",
      "worked for  xgboost_multi_model_endpoint_home_value.ipynb\n",
      "broke on  mxnet_mnist.ipynb\n",
      "worked for  using_r_with_amazon_sagemaker.ipynb\n",
      "worked for  example_r_notebook.ipynb\n",
      "broke on  parquet_to_recordio_protobuf.ipynb\n",
      "broke on  ml_experiment_management_using_search.ipynb\n",
      "worked for  AutoGluon_Tabular_SageMaker.ipynb\n",
      "broke on  inference_pipeline_sparkml_xgboost_car_evaluation.ipynb\n",
      "worked for  sklearn_multi_model_endpoint_home_value.ipynb\n",
      "broke on  linear_learner_multi_model_endpoint_inf_pipeline.ipynb\n",
      "worked for  kmeans_bring_your_own_model.ipynb\n",
      "worked for  scikit_bring_your_own.ipynb\n",
      "worked for  fairseq_sagemaker_translate_de2en.ipynb\n",
      "worked for  fairseq_sagemaker_distributed_translate_de2en.ipynb\n",
      "worked for  fairseq_sagemaker_pretrained_en2fr.ipynb\n",
      "worked for  fairseq_sagemaker_translate_en2fr.ipynb\n",
      "worked for  tensorflow_bring_your_own.ipynb\n",
      "broke on  handling_kms_encrypted_data.ipynb\n",
      "worked for  working_with_redshift_data.ipynb\n",
      "worked for  multi_model_endpoint_bring_your_own.ipynb\n",
      "broke on  data_distribution_types.ipynb\n",
      "worked for  inference_pipeline_sparkml_xgboost_abalone.ipynb\n",
      "broke on  script-mode-container-2.ipynb\n",
      "broke on  framework-container.ipynb\n",
      "broke on  basic_training_container.ipynb\n",
      "broke on  script-mode-container.ipynb\n",
      "worked for  tensorflow_BYOM_iris.ipynb\n",
      "worked for  xgboost_customer_churn_studio.ipynb\n",
      "worked for  pytorch_torchvision_neo_studio.ipynb\n",
      "broke on  tensorflow_distributed_mnist_neo_inf1_studio.ipynb\n",
      "worked for  Image-classification-fulltraining-highlevel-neo-studio.ipynb\n",
      "broke on  xgboost_customer_churn_neo_studio.ipynb\n",
      "worked for  pytorch-vgg19-bn-studio.ipynb\n",
      "worked for  gluoncv_ssd_mobilenet_neo_studio.ipynb\n",
      "worked for  tensorflow_distributed_mnist_neo_studio.ipynb\n",
      "broke on  xgboost_abalone_dist_script_mode.ipynb\n",
      "broke on  xgboost_managed_spot_training.ipynb\n",
      "broke on  xgboost_parquet_input_training.ipynb\n",
      "broke on  xgboost_abalone.ipynb\n",
      "worked for  Scikit-learn Estimator Example With Batch Transform.ipynb\n",
      "worked for  pytorch_cnn_cifar10.ipynb\n",
      "worked for  mxnet_sentiment_analysis_with_gluon.ipynb\n",
      "worked for  mxnet_mnist_with_batch_transform.ipynb\n",
      "worked for  tensorflow_mnist.ipynb\n",
      "worked for  mxnet_onnx_ei.ipynb\n",
      "worked for  keras_pipe_mode_horovod_cifar10.ipynb\n",
      "worked for  Image-classification-transfer-learning-highlevel.ipynb\n",
      "broke on  Image-classification-fulltraining.ipynb\n",
      "worked for  Image-classification-lst-format.ipynb\n",
      "broke on  Image-classification-fulltraining-highlevel.ipynb\n",
      "broke on  Image-classification-transfer-learning.ipynb\n",
      "broke on  Image-classification-lst-format-highlevel.ipynb\n",
      "worked for  linear_learner_mnist.ipynb\n",
      "broke on  mnist-handwritten-digits-classification-experiment.ipynb\n",
      "worked for  track-an-airflow-workflow.ipynb\n",
      "broke on  autopilot_customer_churn.ipynb\n",
      "broke on  autopilot_customer_churn_high_level_with_evaluation.ipynb\n",
      "worked for  sagemaker_autopilot_direct_marketing.ipynb\n",
      "broke on  mnist-handwritten-digits-classification-experiment.ipynb\n",
      "worked for  track-an-airflow-workflow.ipynb\n",
      "worked for  hpo_bring_your_own_keras_container.ipynb\n",
      "worked for  tune_r_bring_your_own.ipynb\n",
      "broke on  hpo_image_classification_warmstart.ipynb\n",
      "broke on  hyperparameter_tuning_mxnet_gluon_cifar10_random_search.ipynb\n",
      "worked for  hpo_mxnet_mnist.ipynb\n",
      "worked for  hpo_xgboost_direct_marketing_sagemaker_python_sdk.ipynb\n",
      "worked for  hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb\n",
      "worked for  HPO_Analyze_TuningJob_Results.ipynb\n",
      "worked for  hpo_tensorflow_mnist.ipynb\n",
      "broke on  hpo_image_classification_early_stopping.ipynb\n",
      "worked for  hpo_pytorch_mnist.ipynb\n",
      "worked for  hpo_xgboost_random_log.ipynb\n",
      "worked for  chainer_single_machine_cifar10.ipynb\n",
      "worked for  Bring_Your_Own-Creating_Algorithm_and_Model_Package.ipynb\n",
      "worked for  automating_auto_insurance_claim_processing.ipynb\n",
      "worked for  Using_ModelPackage_Arn_From_AWS_Marketplace.ipynb\n",
      "broke on  Extracting_insights_from_your_credit_card_statement.ipynb\n",
      "worked for  A_generic_sample_notebook_to_perform_inference_on_ML_model_packages_from_AWS_Marketplace.ipynb\n",
      "broke on  improving_industrial_workplace_safety.ipynb\n",
      "broke on  title_of_your_product-Model.ipynb\n",
      "broke on  title_of_your_product-Algorithm.ipynb\n",
      "worked for  AutoML_-_Train_multiple_models_in_parallel.ipynb\n",
      "worked for  Using_Algorithm_Arn_From_AWS_Marketplace.ipynb\n",
      "worked for  autogluon_tabular_marketplace.ipynb\n",
      "broke on  a_b_testing.ipynb\n",
      "broke on  linear_learner_class_weights_loss_functions.ipynb\n",
      "broke on  linear_learner_multiclass_classification.ipynb\n",
      "broke on  LDA-Science.ipynb\n",
      "broke on  ntm_wikitext.ipynb\n",
      "worked for  streamingMedian.py.ipynb\n",
      "broke on  SageMaker-ModelMonitoring.ipynb\n",
      "worked for  SageMaker-Model-Monitor-Visualize.ipynb\n",
      "worked for  SageMaker-Enable-Model-Monitor.ipynb\n",
      "broke on  ACSBlogPost.ipynb\n",
      "broke on  object_detection_tutorial.ipynb\n",
      "worked for  data_analysis_of_ground_truth_image_classification_output.ipynb\n",
      "broke on  3D-point-cloud-input-data-processing.ipynb\n",
      "broke on  create-3D-pointcloud-labeling-job.ipynb\n",
      "broke on  pretrained_model_labeling_tutorial.ipynb\n",
      "worked for  Identify Worker Accuracy.ipynb\n",
      "broke on  bring_your_own_model_for_sagemaker_labeling_workflows_with_active_learning.ipynb\n",
      "worked for  from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification.ipynb\n",
      "worked for  object_detection_augmented_manifest_training.ipynb\n",
      "worked for  automate_model_retraining_workflow.ipynb\n",
      "worked for  hello_world_workflow.ipynb\n",
      "worked for  training_pipeline_pytorch_mnist.ipynb\n",
      "worked for  machine_learning_workflow_abalone.ipynb\n",
      "Got 182 hits out of 294 total.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# utils is a custom packaged I developed for this project\n",
    "from utils import parse_notebook, initialize_output\n",
    "\n",
    "def main(root_file, verbose, output_file, bucket, path):\n",
    "    '''\n",
    "    Takes a root file name, loops through all files.\n",
    "        When it finds an ipython notebook, pulls in for parsing.\n",
    "    '''\n",
    "    hits = 0\n",
    "    totals = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_file):\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            if '.ipynb' in file:\n",
    "                    \n",
    "                try:\n",
    "                    parse_notebook(input_file = os.path.join(subdir, file), output_file = output_file, bucket=bucket, path=path )\n",
    "                    if verbose:\n",
    "                        print ('worked for ', file)\n",
    "                    hits += 1\n",
    "                except:\n",
    "                    if verbose:\n",
    "                        print ('broke on ', file)\n",
    "                        \n",
    "                totals += 1\n",
    "                         \n",
    "    print ('Got {} hits out of {} total.'.format(hits, totals))\n",
    "    return\n",
    "\n",
    "output_file = \"sagemaker-examples.txt\"\n",
    "initialize_output(output_file)    \n",
    "verbose = True\n",
    "main('amazon-sagemaker-examples', verbose , output_file, bucket, path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's copy that over to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_path = 's3://{}/{}/'.format(bucket, path)\n",
    "# os.system('aws s3 cp {} {}'.format(output_file, s3_train_file) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's format a Python script that imports the large GPT-2 model, points to a fine-tuning framework, and applies our data on this model. \n",
    "\n",
    "Turns out we need a legacy version of TensorFlow to use this fine-tuning framework. In addition, when using script mode on this version of TensorFlow, we actually need to point to a bash script in the SageMaker training container to install our extra pacakges. Let's get that defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/bash_start.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/bash_start.sh\n",
    "\n",
    "# install the extra packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# run our script\n",
    "python tune_gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "# tensorflow=1.14\n",
    "awscli\n",
    "gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, here's a modification of Max Woolf's nice gpt-2-simple package to fine tune GPT2 on a data file we bring ourselves. Thanks for the start code Max!\n",
    "- https://github.com/minimaxir/gpt-2-simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/tune_gpt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tune_gpt.py\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "def get_model_name(model_size):\n",
    "    if 'large' in model_size:\n",
    "        model_name = '774M'\n",
    "\n",
    "    elif 'medium' in model_size: \n",
    "        model_name = \"355M\"\n",
    "\n",
    "    elif 'small' in model_size:\n",
    "        model_name = \"124M\"\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "def get_file_name(bucket, path, file_name):\n",
    "    \n",
    "    # for the exceedingly time-constrained    \n",
    "    os.system('aws s3 cp s3://{}/{}/{} .'.format(bucket, path, file_name))\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def save_to_s3(txt, bucket, path, out_file):\n",
    "    \n",
    "    # say hello to cloudwatch\n",
    "    print (txt)\n",
    "\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write(txt)        \n",
    "\n",
    "    os.system('aws s3 cp {} s3://{}/{}/output/'.format(out_file, bucket, path))\n",
    "        \n",
    "    # could also save the trained model to s3 here\n",
    "    save_path = os.environ.get('SM_MODEL_DIR')\n",
    "    model.save_weights(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "            \n",
    "    # turns out we need to hard code this in twice, once above, and another in the script here due to how the magic function %%writefile was implemented. \n",
    "    bucket = 'your-bucket'\n",
    "    \n",
    "    path = 'your-prefix'\n",
    "        \n",
    "    model_name = get_model_name('large')\n",
    "\n",
    "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "        print(f\"Downloading {model_name} model...\")\n",
    "        gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/model_name/\n",
    "\n",
    "    file_name = get_file_name(bucket, path, 'sagemaker-examples.txt')\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    print ('fine tuning on {}'.format(file_name))\n",
    "    \n",
    "    gpt2.finetune(sess,\n",
    "                  file_name,\n",
    "                  model_name=model_name,\n",
    "                  steps=1000)   # steps is max number of training steps\n",
    "\n",
    "    txt = gpt2.generate(sess, return_as_list = True)[0]\n",
    "    \n",
    "    save_to_s3(txt, bucket, path, 'output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've gotten that file written, just run your job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "est = TensorFlow(entry_point='bash_start.sh',\n",
    "                             role=role,\n",
    "                             source_dir = 'src',\n",
    "                             train_instance_count=1,\n",
    "                             \n",
    "                             # most accounts will need to explicitly request a limit increase for a GPU this large. \n",
    "                             # just reach out to AWS support for this\n",
    "                             train_instance_type='ml.p3dn.24xlarge',\n",
    "                             framework_version='1.14',\n",
    "                             py_version='py3')\n",
    "\n",
    "# feel free to set wait to True here, or logs to True, if you want to see the results here.\n",
    "# Otherwise, wait a few minutes, then open up cloudwatch to view your model training. \n",
    "est.fit(s3_train_path, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
